{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Essgraster.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poornaditya1/Essgraster/blob/master/Essgraster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xPoSNb1yo-G"
      },
      "source": [
        "Dataset : https://www.kaggle.com/c/asap-aes/data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_RcSvGduowX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5cd7a2d-5d0d-4450-b8ad-44ccd91d9c97"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wZz23scH34D"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
        "from keras.models import Sequential, load_model, model_from_config\n",
        "import keras.backend as K\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import math\n",
        "from gensim.test.utils import datapath"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMS58fOcBf5_"
      },
      "source": [
        "#X = pd.read_csv('/content/drive/MyDrive/contents/training_set_rel3.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "X = pd.read_csv('/content/drive/MyDrive/Cicada3301/training_set_rel3.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "y = X['domain1_score']\n",
        "X = X.dropna(axis=1)\n",
        "X = X.drop(columns=['rater1_domain1', 'rater2_domain1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "hatubxCIu4FQ",
        "outputId": "ce3108b7-519d-4302-944a-37d3b578b0ff"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay_id</th>\n",
              "      <th>essay_set</th>\n",
              "      <th>essay</th>\n",
              "      <th>domain1_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear local newspaper, I think effects computer...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   essay_id  ...  domain1_score\n",
              "0         1  ...              8\n",
              "1         2  ...              9\n",
              "2         3  ...              7\n",
              "3         4  ...             10\n",
              "4         5  ...              8\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w91LDnAUu7g7"
      },
      "source": [
        "minimum_scores = [-1, 2, 1, 0, 0, 0, 0, 0, 0]\n",
        "maximum_scores = [-1, 12, 6, 3, 3, 4, 4, 30, 60]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGvboY-CvHA5"
      },
      "source": [
        "def get_model():\n",
        "    \"\"\"Define the model.\"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n",
        "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='relu'))\n",
        "\n",
        "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJEU5IDJC14E"
      },
      "source": [
        "All function definition part will come here\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt59UZk0vOag",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8259b3e7-775b-4e66-e95e-d10e4ee9b01a"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm2q3eezCFAm"
      },
      "source": [
        "#I have added a text to indicate where this block has to be placed\n",
        "def essay_to_wordlist(essay_v, remove_stopwords):\n",
        "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
        "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
        "    words = essay_v.lower().split()\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "    return (words)\n",
        "\n",
        "def essay_to_sentences(essay_v, remove_stopwords):\n",
        "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
        "    return sentences\n",
        "\n",
        "def makeFeatureVec(words, model, num_features):\n",
        "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
        "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
        "    num_words = 0.\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            num_words += 1\n",
        "            featureVec = np.add(featureVec,model[word])        \n",
        "    featureVec = np.divide(featureVec,num_words)\n",
        "    return featureVec\n",
        "\n",
        "def getAvgFeatureVecs(essays, model, num_features):\n",
        "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
        "    counter = 0\n",
        "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
        "    for essay in essays:\n",
        "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
        "        counter = counter + 1\n",
        "    return essayFeatureVecs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpGN6_nevJ9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbe6a6f0-b2c8-4b3f-f27d-207c469f6214"
      },
      "source": [
        "cv = KFold(n_splits=5, shuffle=True)\n",
        "results = []\n",
        "y_pred_list = []\n",
        "\n",
        "count = 1\n",
        "for traincv, testcv in cv.split(X):\n",
        "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
        "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
        "    \n",
        "    train_essays = X_train['essay']\n",
        "    test_essays = X_test['essay']\n",
        "    \n",
        "    sentences = []\n",
        "    \n",
        "    for essay in train_essays:\n",
        "            # Obtaining all sentences from the training essays.\n",
        "            sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
        "            \n",
        "    # Initializing variables for word2vec model.\n",
        "    num_features = 300 \n",
        "    min_word_count = 40\n",
        "    num_workers = 4\n",
        "    context = 10\n",
        "    downsampling = 1e-3\n",
        "\n",
        "    print(\"Training Word2Vec Model...\")\n",
        "    model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
        "\n",
        "    model.init_sims(replace=True)\n",
        "    model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
        "    model.save(\"/content/drive/MyDrive/Cicada3301/model_weights/word2vec.model\")\n",
        "\n",
        "    clean_train_essays = []\n",
        "    \n",
        "    # Generate training and testing data word vectors.\n",
        "    for essay_v in train_essays:\n",
        "        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
        "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
        "    \n",
        "    clean_test_essays = []\n",
        "    for essay_v in test_essays:\n",
        "        clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n",
        "    testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
        "    \n",
        "    trainDataVecs = np.array(trainDataVecs)\n",
        "    testDataVecs = np.array(testDataVecs)\n",
        "    # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
        "    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
        "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
        "    \n",
        "    lstm_model = get_model()\n",
        "    lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=50)\n",
        "    #lstm_model.load_weights('./model_weights/final_lstm.h5')\n",
        "    y_pred = lstm_model.predict(testDataVecs)\n",
        "    \n",
        "    # Save any one of the 8 models.\n",
        "    if count == 5:\n",
        "         lstm_model.save('/content/drive/MyDrive/Cicada3301/model_weights/final_lstm.h5')\n",
        "    \n",
        "    # Round y_pred to the nearest integer.\n",
        "    y_pred = np.around(y_pred)\n",
        "    \n",
        "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
        "    result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
        "    print(\"Kappa Score: {}\".format(result))\n",
        "    results.append(result)\n",
        "\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "--------Fold 1--------\n",
            "\n",
            "Training Word2Vec Model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "163/163 [==============================] - 28s 13ms/step - loss: 78.4444 - mae: 4.9166\n",
            "Epoch 2/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 40.8703 - mae: 3.5867\n",
            "Epoch 3/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 34.0474 - mae: 3.4744\n",
            "Epoch 4/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 33.0127 - mae: 3.4739\n",
            "Epoch 5/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 28.4900 - mae: 3.2552\n",
            "Epoch 6/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 28.3655 - mae: 3.1463\n",
            "Epoch 7/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 25.8646 - mae: 2.9631\n",
            "Epoch 8/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 22.9190 - mae: 2.7482\n",
            "Epoch 9/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 19.5952 - mae: 2.5502\n",
            "Epoch 10/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 18.0518 - mae: 2.4434\n",
            "Epoch 11/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 16.9010 - mae: 2.3153\n",
            "Epoch 12/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 15.9258 - mae: 2.2519\n",
            "Epoch 13/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 14.3252 - mae: 2.1257\n",
            "Epoch 14/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 13.3608 - mae: 2.0938\n",
            "Epoch 15/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 12.8376 - mae: 2.0400\n",
            "Epoch 16/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 12.6061 - mae: 2.0127\n",
            "Epoch 17/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 12.3472 - mae: 2.0044\n",
            "Epoch 18/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 10.8225 - mae: 1.8788\n",
            "Epoch 19/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 11.6436 - mae: 1.9358\n",
            "Epoch 20/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 11.0478 - mae: 1.8904\n",
            "Epoch 21/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 11.0330 - mae: 1.8436\n",
            "Epoch 22/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 11.6328 - mae: 1.8716\n",
            "Epoch 23/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.7287 - mae: 1.8231\n",
            "Epoch 24/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 11.3646 - mae: 1.8407\n",
            "Epoch 25/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.6707 - mae: 1.7945\n",
            "Epoch 26/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.0843 - mae: 1.7780\n",
            "Epoch 27/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.0827 - mae: 1.7696\n",
            "Epoch 28/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.5241 - mae: 1.7182\n",
            "Epoch 29/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.8397 - mae: 1.7436\n",
            "Epoch 30/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.7320 - mae: 1.7451\n",
            "Epoch 31/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.5609 - mae: 1.7295\n",
            "Epoch 32/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.4557 - mae: 1.6946\n",
            "Epoch 33/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.6317 - mae: 1.7457\n",
            "Epoch 34/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.1981 - mae: 1.7501\n",
            "Epoch 35/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.3867 - mae: 1.7411\n",
            "Epoch 36/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.0088 - mae: 1.6956\n",
            "Epoch 37/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.1248 - mae: 1.6799\n",
            "Epoch 38/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.1934 - mae: 1.7452\n",
            "Epoch 39/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.2895 - mae: 1.6546\n",
            "Epoch 40/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.9362 - mae: 1.6810\n",
            "Epoch 41/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.4241 - mae: 1.6255\n",
            "Epoch 42/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.9635 - mae: 1.6570\n",
            "Epoch 43/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.1977 - mae: 1.6233\n",
            "Epoch 44/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.3454 - mae: 1.6526\n",
            "Epoch 45/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 8.2586 - mae: 1.6063\n",
            "Epoch 46/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.3493 - mae: 1.6229\n",
            "Epoch 47/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.0253 - mae: 1.6012\n",
            "Epoch 48/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.4279 - mae: 1.6409\n",
            "Epoch 49/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.5895 - mae: 1.6384\n",
            "Epoch 50/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.5660 - mae: 1.6379\n",
            "Kappa Score: 0.9576169697517044\n",
            "\n",
            "--------Fold 2--------\n",
            "\n",
            "Training Word2Vec Model...\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_2 (LSTM)                (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "163/163 [==============================] - 7s 13ms/step - loss: 85.7821 - mae: 5.0732\n",
            "Epoch 2/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 42.8956 - mae: 3.7101\n",
            "Epoch 3/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 36.7409 - mae: 3.5765\n",
            "Epoch 4/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 31.2419 - mae: 3.3976\n",
            "Epoch 5/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 30.8903 - mae: 3.3733\n",
            "Epoch 6/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 26.7723 - mae: 3.0732\n",
            "Epoch 7/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 27.3562 - mae: 2.9910\n",
            "Epoch 8/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 22.4977 - mae: 2.7776\n",
            "Epoch 9/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 21.3796 - mae: 2.6442\n",
            "Epoch 10/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 17.5328 - mae: 2.4255\n",
            "Epoch 11/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 16.8769 - mae: 2.3472\n",
            "Epoch 12/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 15.9190 - mae: 2.2691\n",
            "Epoch 13/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 15.5739 - mae: 2.2073\n",
            "Epoch 14/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 13.2163 - mae: 2.1136\n",
            "Epoch 15/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 13.8174 - mae: 2.0842\n",
            "Epoch 16/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 13.3639 - mae: 2.0623\n",
            "Epoch 17/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 12.3776 - mae: 1.9918\n",
            "Epoch 18/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 12.9141 - mae: 2.0177\n",
            "Epoch 19/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 12.1080 - mae: 1.9445\n",
            "Epoch 20/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 12.1582 - mae: 1.9284\n",
            "Epoch 21/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 11.8992 - mae: 1.9435\n",
            "Epoch 22/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 11.2124 - mae: 1.8518\n",
            "Epoch 23/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 11.6549 - mae: 1.8755\n",
            "Epoch 24/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.3700 - mae: 1.7869\n",
            "Epoch 25/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 11.1370 - mae: 1.8218\n",
            "Epoch 26/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 11.4525 - mae: 1.8045\n",
            "Epoch 27/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.3048 - mae: 1.7737\n",
            "Epoch 28/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.3438 - mae: 1.7560\n",
            "Epoch 29/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.9179 - mae: 1.7385\n",
            "Epoch 30/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.6280 - mae: 1.7931\n",
            "Epoch 31/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 11.0685 - mae: 1.8348\n",
            "Epoch 32/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.0951 - mae: 1.7394\n",
            "Epoch 33/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.3113 - mae: 1.7500\n",
            "Epoch 34/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.9303 - mae: 1.6563\n",
            "Epoch 35/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 9.7696 - mae: 1.7328\n",
            "Epoch 36/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.3320 - mae: 1.7730\n",
            "Epoch 37/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 9.1583 - mae: 1.6800\n",
            "Epoch 38/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 9.5024 - mae: 1.7146\n",
            "Epoch 39/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.4498 - mae: 1.6819\n",
            "Epoch 40/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.3935 - mae: 1.7454\n",
            "Epoch 41/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.9170 - mae: 1.6820\n",
            "Epoch 42/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.0491 - mae: 1.7028\n",
            "Epoch 43/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.7986 - mae: 1.6661\n",
            "Epoch 44/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.0710 - mae: 1.6837\n",
            "Epoch 45/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.9217 - mae: 1.6753\n",
            "Epoch 46/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.0959 - mae: 1.6578\n",
            "Epoch 47/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.3745 - mae: 1.6413\n",
            "Epoch 48/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.2949 - mae: 1.7084\n",
            "Epoch 49/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.0530 - mae: 1.6563\n",
            "Epoch 50/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.2794 - mae: 1.6775\n",
            "Kappa Score: 0.9638446576678183\n",
            "\n",
            "--------Fold 3--------\n",
            "\n",
            "Training Word2Vec Model...\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_4 (LSTM)                (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "163/163 [==============================] - 7s 13ms/step - loss: 84.6717 - mae: 5.1194\n",
            "Epoch 2/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 45.2180 - mae: 3.7017\n",
            "Epoch 3/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 36.0392 - mae: 3.5119\n",
            "Epoch 4/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 30.6960 - mae: 3.3613\n",
            "Epoch 5/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 29.6188 - mae: 3.3050\n",
            "Epoch 6/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 28.0759 - mae: 3.1741\n",
            "Epoch 7/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 26.6041 - mae: 2.9776\n",
            "Epoch 8/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 24.6263 - mae: 2.8477\n",
            "Epoch 9/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 20.4896 - mae: 2.6081\n",
            "Epoch 10/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 18.4247 - mae: 2.4054\n",
            "Epoch 11/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 15.4045 - mae: 2.1959\n",
            "Epoch 12/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 16.9333 - mae: 2.2948\n",
            "Epoch 13/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 14.0285 - mae: 2.1455\n",
            "Epoch 14/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 14.7396 - mae: 2.1920\n",
            "Epoch 15/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 13.6969 - mae: 2.0716\n",
            "Epoch 16/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 13.5741 - mae: 2.0704\n",
            "Epoch 17/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 12.4960 - mae: 1.9842\n",
            "Epoch 18/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 13.4513 - mae: 1.9979\n",
            "Epoch 19/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 11.7444 - mae: 1.9395\n",
            "Epoch 20/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 11.4836 - mae: 1.8910\n",
            "Epoch 21/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 12.0056 - mae: 1.9031\n",
            "Epoch 22/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 11.1971 - mae: 1.8437\n",
            "Epoch 23/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 11.6356 - mae: 1.8608\n",
            "Epoch 24/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 12.2585 - mae: 1.9395\n",
            "Epoch 25/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.5529 - mae: 1.8087\n",
            "Epoch 26/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.9219 - mae: 1.8340\n",
            "Epoch 27/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.4683 - mae: 1.7810\n",
            "Epoch 28/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 11.7155 - mae: 1.8251\n",
            "Epoch 29/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.4997 - mae: 1.7974\n",
            "Epoch 30/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.0305 - mae: 1.7860\n",
            "Epoch 31/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.2287 - mae: 1.7617\n",
            "Epoch 32/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.0643 - mae: 1.7652\n",
            "Epoch 33/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.8035 - mae: 1.6704\n",
            "Epoch 34/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.8273 - mae: 1.7517\n",
            "Epoch 35/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.2270 - mae: 1.7710\n",
            "Epoch 36/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.1283 - mae: 1.7021\n",
            "Epoch 37/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.2509 - mae: 1.7403\n",
            "Epoch 38/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.1268 - mae: 1.7329\n",
            "Epoch 39/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.0094 - mae: 1.7079\n",
            "Epoch 40/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.1949 - mae: 1.7172\n",
            "Epoch 41/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.3801 - mae: 1.7345\n",
            "Epoch 42/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 9.9575 - mae: 1.7420\n",
            "Epoch 43/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 9.4177 - mae: 1.7002\n",
            "Epoch 44/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.5079 - mae: 1.7069\n",
            "Epoch 45/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.9036 - mae: 1.6723\n",
            "Epoch 46/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.2464 - mae: 1.7438\n",
            "Epoch 47/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.9874 - mae: 1.6768\n",
            "Epoch 48/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.5363 - mae: 1.6409\n",
            "Epoch 49/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.6695 - mae: 1.6660\n",
            "Epoch 50/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.7374 - mae: 1.6789\n",
            "Kappa Score: 0.9610714569551251\n",
            "\n",
            "--------Fold 4--------\n",
            "\n",
            "Training Word2Vec Model...\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_6 (LSTM)                (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "163/163 [==============================] - 7s 13ms/step - loss: 79.1298 - mae: 4.9763\n",
            "Epoch 2/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 45.1890 - mae: 3.7315\n",
            "Epoch 3/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 34.7755 - mae: 3.4844\n",
            "Epoch 4/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 32.0204 - mae: 3.4634\n",
            "Epoch 5/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 29.3383 - mae: 3.3016\n",
            "Epoch 6/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 28.2316 - mae: 3.1382\n",
            "Epoch 7/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 25.2082 - mae: 2.9083\n",
            "Epoch 8/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 22.4452 - mae: 2.7113\n",
            "Epoch 9/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 18.1339 - mae: 2.4469\n",
            "Epoch 10/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 17.2198 - mae: 2.3678\n",
            "Epoch 11/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 16.2724 - mae: 2.2772\n",
            "Epoch 12/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 14.4336 - mae: 2.1597\n",
            "Epoch 13/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 14.3476 - mae: 2.1310\n",
            "Epoch 14/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 14.0644 - mae: 2.1230\n",
            "Epoch 15/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 13.9844 - mae: 2.1121\n",
            "Epoch 16/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 12.7669 - mae: 2.0023\n",
            "Epoch 17/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 12.4151 - mae: 1.9747\n",
            "Epoch 18/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 12.1724 - mae: 1.9642\n",
            "Epoch 19/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 11.0192 - mae: 1.8726\n",
            "Epoch 20/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 12.0927 - mae: 1.9223\n",
            "Epoch 21/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.8564 - mae: 1.8422\n",
            "Epoch 22/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.1685 - mae: 1.8089\n",
            "Epoch 23/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.9766 - mae: 1.8518\n",
            "Epoch 24/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.9637 - mae: 1.8305\n",
            "Epoch 25/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 10.1910 - mae: 1.8045\n",
            "Epoch 26/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.5138 - mae: 1.8121\n",
            "Epoch 27/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.8675 - mae: 1.7434\n",
            "Epoch 28/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.7307 - mae: 1.7705\n",
            "Epoch 29/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.4229 - mae: 1.8013\n",
            "Epoch 30/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 11.1796 - mae: 1.8400\n",
            "Epoch 31/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.6642 - mae: 1.7405\n",
            "Epoch 32/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.4675 - mae: 1.7476\n",
            "Epoch 33/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.0118 - mae: 1.7058\n",
            "Epoch 34/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.8055 - mae: 1.6798\n",
            "Epoch 35/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.5500 - mae: 1.6744\n",
            "Epoch 36/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 9.1894 - mae: 1.7027\n",
            "Epoch 37/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.6723 - mae: 1.7218\n",
            "Epoch 38/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 10.2239 - mae: 1.7315\n",
            "Epoch 39/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.1449 - mae: 1.6961\n",
            "Epoch 40/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 8.9669 - mae: 1.7091\n",
            "Epoch 41/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.7855 - mae: 1.6416\n",
            "Epoch 42/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.1280 - mae: 1.6237\n",
            "Epoch 43/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 7.6694 - mae: 1.6051\n",
            "Epoch 44/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.1002 - mae: 1.6491\n",
            "Epoch 45/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.6977 - mae: 1.6526\n",
            "Epoch 46/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 7.9148 - mae: 1.6077\n",
            "Epoch 47/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.1226 - mae: 1.6083\n",
            "Epoch 48/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.4223 - mae: 1.6611\n",
            "Epoch 49/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.0791 - mae: 1.6845\n",
            "Epoch 50/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.5286 - mae: 1.7076\n",
            "Kappa Score: 0.9578106728818799\n",
            "\n",
            "--------Fold 5--------\n",
            "\n",
            "Training Word2Vec Model...\n",
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_8 (LSTM)                (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_9 (LSTM)                (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "163/163 [==============================] - 8s 13ms/step - loss: 87.3958 - mae: 5.1850\n",
            "Epoch 2/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 41.0699 - mae: 3.6020\n",
            "Epoch 3/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 37.2512 - mae: 3.6460\n",
            "Epoch 4/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 32.2736 - mae: 3.4823\n",
            "Epoch 5/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 27.7643 - mae: 3.2008\n",
            "Epoch 6/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 24.8900 - mae: 2.9395\n",
            "Epoch 7/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 24.8895 - mae: 2.9225\n",
            "Epoch 8/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 20.7654 - mae: 2.6338\n",
            "Epoch 9/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 19.9379 - mae: 2.5672\n",
            "Epoch 10/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 16.3296 - mae: 2.3306\n",
            "Epoch 11/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 16.2701 - mae: 2.3168\n",
            "Epoch 12/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 14.5736 - mae: 2.2070\n",
            "Epoch 13/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 14.1225 - mae: 2.1252\n",
            "Epoch 14/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 13.0136 - mae: 2.0713\n",
            "Epoch 15/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 13.4727 - mae: 2.0778\n",
            "Epoch 16/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 12.8733 - mae: 2.0156\n",
            "Epoch 17/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 13.3867 - mae: 2.0197\n",
            "Epoch 18/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 11.4395 - mae: 1.9094\n",
            "Epoch 19/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 11.9090 - mae: 1.9006\n",
            "Epoch 20/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 11.7727 - mae: 1.9027\n",
            "Epoch 21/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.7629 - mae: 1.8296\n",
            "Epoch 22/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 11.1538 - mae: 1.8626\n",
            "Epoch 23/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 11.1280 - mae: 1.8568\n",
            "Epoch 24/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.6959 - mae: 1.8408\n",
            "Epoch 25/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.5191 - mae: 1.7711\n",
            "Epoch 26/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.3924 - mae: 1.8028\n",
            "Epoch 27/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.5756 - mae: 1.7326\n",
            "Epoch 28/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 10.7982 - mae: 1.7928\n",
            "Epoch 29/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 9.6234 - mae: 1.7474\n",
            "Epoch 30/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.7000 - mae: 1.7483\n",
            "Epoch 31/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.2317 - mae: 1.7070\n",
            "Epoch 32/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.6677 - mae: 1.7326\n",
            "Epoch 33/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 9.3019 - mae: 1.7114\n",
            "Epoch 34/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 9.8708 - mae: 1.7365\n",
            "Epoch 35/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 9.1484 - mae: 1.7037\n",
            "Epoch 36/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.6707 - mae: 1.6692\n",
            "Epoch 37/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 9.7854 - mae: 1.7380\n",
            "Epoch 38/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 9.4059 - mae: 1.7097\n",
            "Epoch 39/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 8.8827 - mae: 1.6761\n",
            "Epoch 40/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 8.2426 - mae: 1.6470\n",
            "Epoch 41/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 8.9476 - mae: 1.6814\n",
            "Epoch 42/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 8.5697 - mae: 1.6454\n",
            "Epoch 43/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 8.9419 - mae: 1.6820\n",
            "Epoch 44/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 8.4004 - mae: 1.6308\n",
            "Epoch 45/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.6739 - mae: 1.6554\n",
            "Epoch 46/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.2261 - mae: 1.6408\n",
            "Epoch 47/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 8.6761 - mae: 1.6286\n",
            "Epoch 48/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 8.2645 - mae: 1.6394\n",
            "Epoch 49/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 8.2799 - mae: 1.6385\n",
            "Epoch 50/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 8.4371 - mae: 1.6277\n",
            "Kappa Score: 0.9421804188627044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlbmwFZrvq8E",
        "outputId": "079d18e1-f32e-4ff9-bcab-242a21105433"
      },
      "source": [
        "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Kappa score after a 5-fold cross validation:  0.959\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCQUxP1gIHSI"
      },
      "source": [
        "contentBad = \"\"\"\n",
        "        In Let there be dark, Paul Bogard talks about the importance of darkness.\n",
        "\n",
        "Darkness is essential to humans. Bogard states, Our bodies need darkness to produce the hormone melatonin, which keeps certain cancers from developing, and our bodies need darkness for sleep, sleep. Sleep disorders have been linked to diabetes, obesity, cardiovascular disease and depression and recent research suggests are main cause of short sleep is long light. Whether we work at night or simply take our tablets, notebooks and smartphones to bed, there isnt a place for this much artificial light in our lives. (Bogard 2). Here, Bogard talks about the importance of darkness to humans. Humans need darkness to sleep in order to be healthy.\n",
        "\n",
        "Animals also need darkness. Bogard states, The rest of the world depends on darkness as well, including nocturnal and crepuscular species of birds, insects, mammals, fish and reptiles. Some examples are well knownthe 400 species of birds that migrate at night in North America, the sea turtles that come ashore to lay their eggsand some are not, such as the bats that save American farmers billions in pest control and the moths that pollinate 80% of the worlds flora. Ecological light pollution is like the bulldozer of the night, wrecking habitat and disrupting ecosystems several billion years in the making. Simply put, without darkness, Earths ecology would collapse... (Bogard 2). Here Bogard explains that animals, too, need darkness to survive.\n",
        "    \"\"\"\n",
        "contentGood = \"\"\"\n",
        "        In response to our worlds growing reliance on artificial light, writer Paul Bogard argues that natural darkness should be preserved in his article Let There be dark. He effectively builds his argument by using a personal anecdote, allusions to art and history, and rhetorical questions.\n",
        "\n",
        "Bogard starts his article off by recounting a personal story  a summer spent on a Minnesota lake where there was woods so dark that [his] hands disappeared before [his] eyes. In telling this brief anecdote, Bogard challenges the audience to remember a time where they could fully amass themselves in natural darkness void of artificial light. By drawing in his readers with a personal encounter about night darkness, the author means to establish the potential for beauty, glamour, and awe-inspiring mystery that genuine darkness can possess. He builds his argument for the preservation of natural darkness by reminiscing for his readers a first-hand encounter that proves the irreplaceable value of darkness. This anecdote provides a baseline of sorts for readers to find credence with the authors claims.\n",
        "\n",
        "Bogards argument is also furthered by his use of allusion to art  Van Goghs Starry Night  and modern history  Paris reputation as The City of Light. By first referencing Starry Night, a painting generally considered to be undoubtedly beautiful, Bogard establishes that the natural magnificence of stars in a dark sky is definite. A world absent of excess artificial light could potentially hold the key to a grand, glorious night sky like Van Goghs according to the writer. This urges the readers to weigh the disadvantages of our world consumed by unnatural, vapid lighting. Furthermore, Bogards alludes to Paris as the famed city of light. He then goes on to state how Paris has taken steps to exercise more sustainable lighting practices. By doing this, Bogard creates a dichotomy between Paris traditionally alluded-to name and the reality of what Paris is becoming  no longer the city of light, but moreso the city of lightbefore 2 AM. This furthers his line of argumentation because it shows how steps can be and are being taken to preserve natural darkness. It shows that even a city that is literally famous for being constantly lit can practically address light pollution in a manner that preserves the beauty of both the city itself and the universe as a whole.\n",
        "\n",
        "Finally, Bogard makes subtle yet efficient use of rhetorical questioning to persuade his audience that natural darkness preservation is essential. He asks the readers to consider what the vision of the night sky might inspire in each of us, in our children or grandchildren? in a way that brutally plays to each of our emotions. By asking this question, Bogard draws out heartfelt ponderance from his readers about the affecting power of an untainted night sky. This rhetorical question tugs at the readers heartstrings; while the reader may have seen an unobscured night skyline before, the possibility that their child or grandchild will never get the chance sways them to see as Bogard sees. This strategy is definitively an appeal to pathos, forcing the audience to directly face an emotionally-charged inquiry that will surely spur some kind of response. By doing this, Bogard develops his argument, adding gutthral power to the idea that the issue of maintaining natural darkness is relevant and multifaceted.\n",
        "\n",
        "Writing as a reaction to his disappointment that artificial light has largely permeated the prescence of natural darkness, Paul Bogard argues that we must preserve true, unaffected darkness. He builds this claim by making use of a personal anecdote, allusions, and rhetorical questioning.\n",
        "    \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYbwmwwi6vEz",
        "outputId": "786affdc-19ba-4fe9-c257-bffbbff6eff8"
      },
      "source": [
        " content = contentBad\n",
        "    \n",
        "if len(content) > 20:\n",
        "  num_features = 300\n",
        "  clean_test_essays = []\n",
        "  clean_test_essays.append(essay_to_wordlist( content, remove_stopwords=True ))\n",
        "  testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
        "  #Error in the above line\n",
        "\n",
        "  #Just upar wale block me error hai probably, rectify kar dena please\n",
        "\n",
        "  \n",
        "  testDataVecs = np.array(testDataVecs)\n",
        "  testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
        "\n",
        "  preds = lstm_model.predict(testDataVecs)\n",
        "  print(preds)\n",
        "\n",
        "  if math.isnan(preds):\n",
        "    preds = 0\n",
        "  else:\n",
        "    preds = np.round(preds)\n",
        "\n",
        "  if preds < 0:\n",
        "    preds = 0\n",
        "  #else:\n",
        "  #  preds = 0\n",
        "        \n",
        "print(int(preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5.391217]]\n",
            "5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deyIzPcm69RY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4b3910b-4156-4d98-be13-3d0bb8fee610"
      },
      "source": [
        "from tensorflow import keras\n",
        "content = contentGood\n",
        "    \n",
        "if len(content) > 20:\n",
        "  num_features = 300\n",
        "  clean_test_essays = []\n",
        "  clean_test_essays.append(essay_to_wordlist( content, remove_stopwords=True ))\n",
        "  testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
        "  #Error in the above line\n",
        "\n",
        "  #Just upar wale block me error hai probably, rectify kar dena please\n",
        "\n",
        "  \n",
        "  testDataVecs = np.array(testDataVecs)\n",
        "  testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
        "\n",
        "  #preds = lstm_model.predict(testDataVecs)\n",
        "  model = keras.models.load_model('/content/drive/MyDrive/Cicada3301/model_weights/final_lstm.h5')\n",
        "  preds = model.predict(testDataVecs)\n",
        "\n",
        "  print(preds)\n",
        "\n",
        "  if math.isnan(preds):\n",
        "    preds = 0\n",
        "  else:\n",
        "    preds = np.round(preds)\n",
        "\n",
        "  if preds < 0:\n",
        "    preds = 0\n",
        "  #else:\n",
        "  #  preds = 0\n",
        "        \n",
        "print(int(preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "2021-05-28 09:17:24.468 WARNING tensorflow: Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "2021-05-28 09:17:24.582 WARNING tensorflow: Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[4.386077]]\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeIate19Cy6W",
        "outputId": "39578768-b479-48a5-e7d6-d16a4862f03a"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-29 04:03:47--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.144.219.72, 35.170.116.11, 34.205.198.58, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.144.219.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ngrok-stable-linux-amd64.zip\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  82.9MB/s    in 0.2s    \n",
            "\n",
            "2021-05-29 04:03:47 (82.9 MB/s) - ngrok-stable-linux-amd64.zip saved [13832437/13832437]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbjjGOBXDkXJ",
        "outputId": "eb99dc2a-d34b-41c5-d761-d4f47cea7f81"
      },
      "source": [
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSl_Qe6zAH9J",
        "outputId": "4160ee38-e492-42ef-87f2-372f833ea03a"
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow import keras\n",
        "import math\n",
        "from gensim.test.utils import datapath\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
        "from keras.models import Sequential, load_model, model_from_config\n",
        "\n",
        "preds = 0\n",
        "n = 0\n",
        "\n",
        "minimum_scores = [-1, 2, 1, 0, 0, 0, 0, 0, 0]\n",
        "maximum_scores = [-1, 12, 6, 3, 3, 4, 4, 30, 60]\n",
        "\n",
        "def essay_to_wordlist(essay_v, remove_stopwords):\n",
        "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
        "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
        "    words = essay_v.lower().split()\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "    return (words)\n",
        "\n",
        "def essay_to_sentences(essay_v, remove_stopwords):\n",
        "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
        "    return sentences\n",
        "\n",
        "def makeFeatureVec(words, model, num_features):\n",
        "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
        "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
        "    num_words = 0.\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            num_words += 1\n",
        "            featureVec = np.add(featureVec,model[word])        \n",
        "    featureVec = np.divide(featureVec,num_words)\n",
        "    return featureVec\n",
        "\n",
        "def getAvgFeatureVecs(essays, model, num_features):\n",
        "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
        "    counter = 0\n",
        "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
        "    for essay in essays:\n",
        "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
        "        counter = counter + 1\n",
        "    return essayFeatureVecs\n",
        "\n",
        "def u_in():\n",
        "  num = st.text_input(\"Enter question number: \")\n",
        "  text = st.text_input(\"Enter your essay to be graded: \")\n",
        "\n",
        "  return text,num\n",
        "\n",
        "def header(url):\n",
        "  st.markdown(f'<p style=\"font-size:50px;border-radius:5%;text-align:center;\">{url}</p>', unsafe_allow_html=True)\n",
        "#  st.markdown(\"__Auto Essay Grader__\")    \n",
        "def header1(url):\n",
        "  st.markdown(f'<p style=\"font-size:26px;border-radius:2%;text-align:center;\">{url}</p>', unsafe_allow_html=True)\n",
        "header(\"Auto Essay Grader\")\n",
        "#st.subheader(\"Questions:\")\n",
        "header1(\"All the Best!!!!!!\")\n",
        "st.title(\"Questions:\")\n",
        "st.write(\"1. Zoos have been amusing and educating humans about animals for centuries. Although, containment of animals in zoos is an increasingly controversial topic. Some argue that confining animals to their cages in zoos are both necessary and healthy, while others refute this. Both sides of this debate will be analysed in this essay before a reasoned conclusion is drawn. Drop your view on this.\")\n",
        "st.write(\"2. If you had the opportunity to bring any person  past or present, fictional or nonfictional  to a place that is special to you (your hometown or country, a favourite location, etc.), who would you bring and why? Tell us what you would share with that person. \")\n",
        "st.write(\"3. The world is becoming more and more globalized which can be considered a very positive thing on the overall but it is important to note that this globalization brings with itself the probability of more and more disagreements and difference in perspectives. Since the invention of nuclear weapons, we have had a long period of GLOBAL peace and stability. Are nuclear weapons global peacemakers or killing devices? \")\n",
        "st.write(\"4. After 23 films in the franchise, Marvel has earned the right to take a huge risk for. What they gave the audience was a profound and impactful case study in the grieving process. If you are an avenger fan and would like to fill some of your thoughts into this context then take a chance to fill below. \")\n",
        "st.write(\"5. Pretend you woke up one day and there is no Covid. People are moving around freely without mask. You see them going on trips. You plan for trips. Explain your thoughts on this imagination of every citizen of what the world would be like. Use your imagination!\")\n",
        "st.write(\"6. Since our nation's founding, the government -- colonial, federal, and state has punished a varying percentage of arbitrarily-selected murders with the ultimate sanction: death. Sentencing remained to be like tit-for-tat. Explain you views on if the death penalty is effective or not?\")\n",
        "st.write(\"7. Imagine that your teacher wants to teach a new subject for the next few weeks. Your teacher will take suggestions, and then let the students vote on the new subject. What subject you as a student choose? Write an essay to support your choice and to persuade the other students to vote for your choice.\")\n",
        "st.write(\"8. The place of black hole in space is such that the gravity pulls a body in such a case that even light cannot pull out with a great amount of matter packed in a small area. You being a space travel enthusiast, write an essay of you travelling to the black hole.\")\n",
        "\n",
        "df,n = u_in()\n",
        "content = df\n",
        "\n",
        "model = Word2Vec.load(\"/content/drive/MyDrive/Cicada3301/model_weights/word2vec.model\")\n",
        "\n",
        "lstm_model = keras.models.load_model('/content/drive/MyDrive/Cicada3301/model_weights/final_lstm.h5')\n",
        "\n",
        "if len(content) > 20:\n",
        "  num_features = 300\n",
        "  clean_test_essays = []\n",
        "  clean_test_essays.append(essay_to_wordlist( content, remove_stopwords=True ))\n",
        "  testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
        "\n",
        "  \n",
        "  testDataVecs = np.array(testDataVecs)\n",
        "  testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
        "\n",
        "  preds = lstm_model.predict(testDataVecs)\n",
        "  \n",
        "\n",
        "\n",
        "  if math.isnan(preds):\n",
        "    preds = 0\n",
        "  else:\n",
        "    preds = np.round(preds)\n",
        "\n",
        "  if preds < 0:\n",
        "    preds = 0\n",
        "  #else:\n",
        "  #  preds = 0\n",
        "if n is not 0:  \n",
        "  st.write(\"Question no. : \" + n)\n",
        "  st.write(\"Final grade is \" + str(int(preds)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y44e_ZnDmw7"
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 8501 &')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIg5G3rtDpDu",
        "outputId": "84a09caa-5feb-4410-eb40-33bce373b01d"
      },
      "source": [
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    'import sys, json; print(\"Execute the next cell and the go to the following URL: \" +json.load(sys.stdin)[\"tunnels\"][0][\"public_url\"])'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Execute the next cell and the go to the following URL: https://b965d6142bdb.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqrRdggLDsYT",
        "outputId": "59b1ff03-16af-46fa-8299-24d51dda8d1b"
      },
      "source": [
        "!streamlit run app.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.2:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.221.61.145:8501\u001b[0m\n",
            "\u001b[0m\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "2021-05-29 05:08:18.355750: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-05-29 05:08:19.487 'pattern' package not found; tag filters are not available for English\n",
            "2021-05-29 05:08:19.490 adding document #0 to Dictionary(0 unique tokens: [])\n",
            "2021-05-29 05:08:19.490 built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n",
            "2021-05-29 05:08:19.940 loading Word2Vec object from /content/drive/MyDrive/Cicada3301/model_weights/word2vec.model\n",
            "2021-05-29 05:08:19.996 loading wv recursively from /content/drive/MyDrive/Cicada3301/model_weights/word2vec.model.wv.* with mmap=None\n",
            "2021-05-29 05:08:19.997 setting ignored attribute vectors_norm to None\n",
            "2021-05-29 05:08:19.997 loading vocabulary recursively from /content/drive/MyDrive/Cicada3301/model_weights/word2vec.model.vocabulary.* with mmap=None\n",
            "2021-05-29 05:08:19.997 loading trainables recursively from /content/drive/MyDrive/Cicada3301/model_weights/word2vec.model.trainables.* with mmap=None\n",
            "2021-05-29 05:08:19.997 setting ignored attribute cum_table to None\n",
            "2021-05-29 05:08:19.997 loaded /content/drive/MyDrive/Cicada3301/model_weights/word2vec.model\n",
            "2021-05-29 05:08:20.010752: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
            "2021-05-29 05:08:20.038452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 05:08:20.039066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-05-29 05:08:20.039134: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-05-29 05:08:20.041393: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-05-29 05:08:20.041480: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-05-29 05:08:20.042974: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
            "2021-05-29 05:08:20.043328: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
            "2021-05-29 05:08:20.044857: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-05-29 05:08:20.045336: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-05-29 05:08:20.045536: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-05-29 05:08:20.045642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 05:08:20.046243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 05:08:20.046781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-05-29 05:08:20.047353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 05:08:20.047891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-05-29 05:08:20.047998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 05:08:20.048620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 05:08:20.049134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-05-29 05:08:20.049203: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-05-29 05:08:20.642823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-05-29 05:08:20.642874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
            "2021-05-29 05:08:20.642890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
            "2021-05-29 05:08:20.643065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 05:08:20.643713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 05:08:20.644278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 05:08:20.644787: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-05-29 05:08:20.644831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13837 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2021-05-29 05:08:20.656 Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "2021-05-29 05:08:20.780 Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgbFdLCfDwcM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "43e89282-f99c-422c-b195-2b534f5158d1"
      },
      "source": [
        "!pip install streamlit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting streamlit\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/11/57097e14f72a2d1b2a1bbe86c2a8bc375661bfd5c30b5e8cee7c2fad9a44/streamlit-0.82.0-py2.py3-none-any.whl (8.2MB)\n",
            "\u001b[K     || 8.2MB 15.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.19.5)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit) (20.9)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.1)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.8.1)\n",
            "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.12.4)\n",
            "Requirement already satisfied: pyarrow; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.0.0)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.1.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.1.0)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/bc/f0e44828e4290367c869591d50d3671a4d0ee94926da6cb734b7b200308c/pydeck-0.6.2-py2.py3-none-any.whl (4.2MB)\n",
            "\u001b[K     || 4.2MB 50.2MB/s \n",
            "\u001b[?25hCollecting blinker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/51/e2a9f3b757eb802f61dc1f2b09c8c99f6eb01cf06416c0671253536517b6/blinker-1.4.tar.gz (111kB)\n",
            "\u001b[K     || 112kB 55.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n",
            "Collecting watchdog; platform_system != \"Darwin\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/5b/36b3b11e557830de6fc1dc06e9aa3ee274119b8cea9cc98175dbbf72cf87/watchdog-2.1.2-py3-none-manylinux2014_x86_64.whl (74kB)\n",
            "\u001b[K     || 81kB 11.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<8.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Collecting base58\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/a1/d9f565e9910c09fd325dc638765e8843a19fa696275c16cc08cf3b0a3c25/base58-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.23.0)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.1.5)\n",
            "Collecting validators\n",
            "  Downloading https://files.pythonhosted.org/packages/db/2f/7fed3ee94ad665ad2c1de87f858f10a7785251ff75b4fd47987888d07ef1/validators-0.18.2-py3-none-any.whl\n",
            "Collecting gitpython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/da/6f6224fdfc47dab57881fe20c0d1bc3122be290198ba0bf26a953a045d92/GitPython-3.1.17-py3-none-any.whl (166kB)\n",
            "\u001b[K     || 174kB 56.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit) (2.4.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->streamlit) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (56.1.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.11.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.6.3)\n",
            "Collecting ipykernel>=5.1.2; python_version >= \"3.4\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/6d/6c8fe4b658f77947d4244ce81f60230c4c8d1dc1a21ae83e63b269339178/ipykernel-5.5.5-py3-none-any.whl (120kB)\n",
            "\u001b[K     || 122kB 51.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.0.5)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tzlocal->streamlit) (2018.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.10)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators->streamlit) (4.4.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     || 71kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from gitpython->streamlit) (3.7.4.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.1.3)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.1)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.5.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (5.3.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.7.1)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.8.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (22.0.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.5.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.10.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.3.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.4.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n",
            "Building wheels for collected packages: blinker\n",
            "  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for blinker: filename=blinker-1.4-cp37-none-any.whl size=13448 sha256=0189837450402b41a6633e0486871b81f8727bc47e873489981a08b52fabb36a\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/a0/00/8690a57883956a301d91cf4ec999cc0b258b01e3f548f86e89\n",
            "Successfully built blinker\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.10, but you'll have ipykernel 5.5.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: ipykernel, pydeck, blinker, watchdog, base58, validators, smmap, gitdb, gitpython, streamlit\n",
            "  Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "Successfully installed base58-2.1.0 blinker-1.4 gitdb-4.0.7 gitpython-3.1.17 ipykernel-5.5.5 pydeck-0.6.2 smmap-4.0.0 streamlit-0.82.0 validators-0.18.2 watchdog-2.1.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ipykernel"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4qF0-KIYtoI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}